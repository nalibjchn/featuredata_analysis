{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e38f483-4755-4e1f-8377-634ec7a0bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wave\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fbc26-4fc2-4093-8404-a0c25ad99d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countfiles(path, filetype):\n",
    "    # show how many video files\n",
    "    allfiles = os.listdir(rootpath)\n",
    "    files = [ fname for fname in allfiles if fname.endswith(filetype)]\n",
    "    # files = [ fname for fname in allfiles if fname.endswith('.mp4')]\n",
    "    print(files[0:5])\n",
    "    print(len(files))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25fa7c-b3e4-41f9-88c0-6e5554a3a12d",
   "metadata": {},
   "source": [
    "### Silence duration time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a475a-a8c8-447e-8f4d-aeb5d2f399b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. labeled pariticpants silence response time by Aduacity\n",
    "2. calculate silence duration time (s) = start - end by google sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecbccf-571e-45a4-ba66-f79e370c7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised silence duration time\n",
    "df_silence_A1_help =  pd.read_csv('/home/nali/Develop/HRI/data/HRI_Study2_result_csv/silence_duration/part2/condition_A1_withhelp/condition_A1_help.csv')\n",
    "# df_silence_B =  pd.read_csv('/home/nali/Develop/HRI/silentduration_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb55b34-91da-49e1-9362-8458b1bffc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise\n",
    "min_silence= df_silence_A1_help['silent_length'].min()\n",
    "max_silence= df_silence_A1_help['silent_length'].max()\n",
    "range_max_min = max_silence - min_silence\n",
    "print(min_silence, max_silence)\n",
    "df_silence_A1_help['norm_silentlength'] = (df_silence_A1_help['silent_length']-min_silence) / range_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce415e-6b28-4b5b-8677-b0efdb59bf55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_silence_A1_help.to_csv('/home/nali/Develop/HRI/data/HRI_Study2_result_csv/silence_duration/part2/condition_A1_withhelp/condition_A1_help.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed0c56-9591-4f14-8cf1-c2c72379cd63",
   "metadata": {},
   "source": [
    "### Only Participants speech extraction\n",
    "1. Labelled participants speech by Aduactiy\n",
    "2. Cut audio chips from labelled files (.txt)\n",
    "3. Concatenate audio chips into one partipcant audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17189d40-9af7-4172-923b-3a37d6ac7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopath = '/media/nali/Seagate Portable Drive/6_HRIStudy2_audioData/part2/condition_A2_B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b4778-2198-4bbb-9346-e285ffea32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile_path = '/media/nali/Seagate Portable Drive/HRI_Study2_result_csv/participantSpeech/Part2_A2_B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c0e28-b3c9-4593-a6e0-d3c0951be024",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = countfiles(audiopath, '.mp3')\n",
    "config_files = countfiles(configfile_path, '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0564b-a45b-486e-96c9-6d1cc8be399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "index = 0\n",
    "for audio in audio_files:\n",
    "    audio_name = audio.split('.')[0]\n",
    "    audio_config = configfile_path + audio_name + '.txt'\n",
    "# Inicialize the extractor\n",
    "    ext = AudioClipExtractor( audiopath + audio, '/usr/bin/ffmpeg')\n",
    "# Extract the clips according to the specs and save them as a zip archive\n",
    "    if(os.path.exists(audio_config)):\n",
    "        ext.extract_clips(audio_config, configfile_path, zip_output=True)\n",
    "        index +=1\n",
    "    else:\n",
    "        print(audio_config)\n",
    "print(\"done!!\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4c6be-2be9-41ac-a657-36bc522fa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  folder name: config_files + clips\n",
    "# step 3\n",
    "# extract zip files of participant audios \n",
    "# concatenate these audio files into one audio for each zipped folder\n",
    "index = 0\n",
    "error = 0\n",
    "for config in config_files:\n",
    "    foldername = config.split('.')[0] + '_clips'\n",
    "    auidofiles = '/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A2_B/' + foldername + '/'\n",
    "    # auidofiles = '/home/nali/Develop/HRI/data/HRI_Study2_result_csv/participantSpeech/79_face_A24_BA2_trim_clips/'\n",
    "    allfiles = os.listdir(auidofiles)\n",
    "    allfiles.sort()\n",
    "    try:\n",
    "        files = [ AudioFileClip(auidofiles + fname) for fname in allfiles if fname.endswith('.mp3')]\n",
    "        final_audio = concatenate_audioclips(files)\n",
    "        final_audio.write_audiofile('/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A2_B/mp3/'\n",
    "                                + config.split('.')[0] + '_user' +'.mp3')\n",
    "    except:\n",
    "        print(\"****error****\", foldername)\n",
    "        error +=1\n",
    "    index += 1\n",
    "\n",
    "print(\"done\", index, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871eacd-a80f-4704-a3da-d35971f96e72",
   "metadata": {},
   "source": [
    "### Audio Signal Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97453cc-ea62-4475-979d-2d423abc716c",
   "metadata": {},
   "source": [
    "#### 1. MFCCs - Mel Frequency Cepstral Coefficients\n",
    "scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59039d5-4ea2-4e52-8056-732bf31c47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import wavfile as sciwav\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc3772-b5c9-4c82-8e29-afe509842403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofiles(path):\n",
    "    all_audio_files = os.listdir(path)\n",
    "    audio_files = [ fname for fname in all_audio_files if fname.endswith('.wav')]\n",
    "    audio_files.sort()\n",
    "    print(audio_files[:10])\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffe46f-cd7a-49d1-9a71-d3c81c7e9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopath = '/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A1_beforehelp/wav/'\n",
    "audiofiles = load_audiofiles(audiopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffdb2c6-0af7-43d5-b2c4-69d108580158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get MMCCs features\n",
    "signal, fs = librosa.load(audiopath + audiofiles[1])\n",
    "s_len = len(signal)\n",
    "\n",
    "if s_len < mean_signal_length:\n",
    "    pad_len = mean_signal_length - s_len\n",
    "    pad_rem = pad_len % 2\n",
    "    pad_len //= 2\n",
    "    signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "else:\n",
    "    pad_len = s_len - mean_signal_length\n",
    "    pad_len //= 2\n",
    "    signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
    "mfcc = mfcc.T\n",
    "feature = mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce933851-d2cb-494c-ba9b-ddd959a5e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature = []\n",
    "mfcc_feature.append([mfcc, audiofiles[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4d8a0-7c17-4658-9748-35680d5a3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_feature_all = {}\n",
    "mfcc_feature_all =  pd.DataFrame(mfcc_feature, columns = ['feature','filename'])\n",
    "mfcc_feature_all.to_csv(\"the path you like.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc177f9-c253-4564-8ea8-a06fcc09816c",
   "metadata": {},
   "source": [
    "#### 2. Pitch\n",
    "\n",
    "pip install AMFM-decompy\n",
    "\n",
    "YAAPT: yet another algorithm for pitch tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c33edd-c0b8-4dcc-ba2f-3ae01e074633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import amfm_decompy.pYAAPT as pYAAPT\n",
    "import amfm_decompy.basic_tools as basic\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948d4c5-c1fc-4c66-9418-d9977c604a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateMono(path, name):\n",
    "    sound = AudioSegment.from_wav(path)\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(output_mono+name, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc3a49-0cf3-4e52-b972-100aaf5edcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mono = '/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A1_B/wav_mono/'\n",
    "folderpath = '/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A1_B/wav/'\n",
    "allfiles = load_audiofiles(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ea338-b0f0-4eda-ba80-86f1e7bb09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for file in allfiles:\n",
    "    name = file.split('.')[0] + '_mono.wav'\n",
    "    updateMono(folderpath + file, name)\n",
    "    if (index%10 == 0):\n",
    "        print(\"mono processing\", index)\n",
    "    index+=1\n",
    "    \n",
    "print(\"done\", index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2594ab-2334-4be3-94ed-954f4f7065ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pitchValues = pd.DataFrame(columns=['file', 'pitch', 'median', 'mean', 'q1', 'q3'])\n",
    "df_pitchValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d205cd4-d5ac-4f26-8a4d-eb6f24e95008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pichvalue(pathfile, index):\n",
    "    signal_new = basic.SignalObj(pathfile)\n",
    "    # print(pathfile)\n",
    "    pitchYaapt_new = pYAAPT.yaapt(signal_new)\n",
    "    df_pitchValues.loc[index] = [pathfile.split('/')[-1], pitchYaapt_new.samp_values,\n",
    "                                 np.median(pitchYaapt_new.samp_values), np.mean(pitchYaapt_new.samp_values),\n",
    "                                 np.percentile(pitchYaapt_new.samp_values, 25, interpolation = 'midpoint'),\n",
    "                                 np.percentile(pitchYaapt_new.samp_values, 75, interpolation = 'midpoint')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c623c-b1b8-4682-8152-a789ab79abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for file in load_audiofiles(output_mono):\n",
    "    \n",
    "    try:\n",
    "        pichvalue(output_mono + file, index)\n",
    "    except:\n",
    "        print(output_mono+ file, index)\n",
    "        \n",
    "    if (index%10 == 0):\n",
    "        print(\"pitch processing\", index)\n",
    "    index +=1\n",
    "    \n",
    "print(\"done\", index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872dfa6-a458-487b-bb70-4a09b3ce136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pitchValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3e92a-da7d-4cbd-bf8a-2efc74e05951",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(df_pitchValues.iloc[0]['pitch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7cb1b-ce8a-4b23-8bdf-08a8ef3a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_pitch = pd.DataFrame(df_pitchValues['pitch'], columns=['pitch'])\n",
    "dp_pitch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63777088-2060-4664-ac05-845db08de3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_csv\n",
    "df_pitchValues.to_csv(\"/home/nali/Develop/HRI/data/HRI_Study2_result_csv/participantSpeech/pitch/part2_A1_B.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
