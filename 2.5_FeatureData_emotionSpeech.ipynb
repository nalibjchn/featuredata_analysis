{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b405f89e-e0e4-461f-a061-ac2707b6d04d",
   "metadata": {},
   "source": [
    "#### Setup environment\n",
    "1. download this model, link: https://github.com/Jiaxin-Ye/TIM-Net_SER.git\n",
    "2. put this file into TIM-Net_SER/Code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bcf597-a59f-4c15-895e-9bb24e1317f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Layer,Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from Common_Model import Common_Model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import wavfile as sciwav\n",
    "from scipy import signal\n",
    "import librosa.display\n",
    "\n",
    "from TIMNET import TIMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10f0ff6-7faf-444e-bd89-16b9bef2a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from Model import TIMNET_Model\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a607fc5-ed67-4ae5-8f3d-d8420de33d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofiles(path):\n",
    "    all_audio_files = os.listdir(path)\n",
    "    audio_files = [ fname for fname in all_audio_files if fname.endswith('.wav')]\n",
    "    audio_files.sort()\n",
    "    print(audio_files[:10])\n",
    "    print(len(audio_files))\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "100a0e5f-68f3-423d-9693-e32bf90741d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11_face_B1_BA1_trim_user.wav', '11_face_B2_BA1_trim_user.wav', '11_face_B3_BA1_trim_user.wav', '11_face_B4_BA1_trim_user.wav', '12_face_B1_BA1_trim_user.wav', '12_face_B2_BA1_trim_user.wav', '12_face_B3_BA1_trim_user.wav', '12_face_B4_BA1_trim_user.wav', '13_face_B1_BA1_trim_user.wav', '13_face_B2_BA1_trim_user.wav']\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "audiopath = '/media/nali/Seagate Portable Drive/8_HRIStudy2_audioData_participantonly/part2_A1_B/wav/'\n",
    "audiofiles = load_audiofiles(audiopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18939742-66bf-4cdc-b17d-2cec70a9d68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38146"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get mean of signal length\n",
    "# A1_beforehelp\n",
    "s_len_sum = 0\n",
    "for audiofile in audiofiles:\n",
    "    signal, fs = librosa.load(audiopath + audiofile)\n",
    "    s_len = len(signal)\n",
    "    s_len_sum += s_len\n",
    "mean_signal_length = int(s_len_sum/len(audiofiles))\n",
    "mean_signal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6891f80-e04f-4e1a-9e39-e9daa8bf9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get MMCCs features --- test\n",
    "# signal, fs = librosa.load(audiopath + audiofiles[1])\n",
    "# s_len = len(signal)\n",
    "\n",
    "# if s_len < mean_signal_length:\n",
    "#     pad_len = mean_signal_length - s_len\n",
    "#     pad_rem = pad_len % 2\n",
    "#     pad_len //= 2\n",
    "#     signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "# else:\n",
    "#     pad_len = s_len - mean_signal_length\n",
    "#     pad_len //= 2\n",
    "#     signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "# mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
    "\n",
    "# feature = np.repeat(mfcc[:, :, np.newaxis], 10, axis=2)\n",
    "# feature = feature.T\n",
    "# mfcc_feature.append([mfcc, audiofiles[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e738546-b063-4a6d-bc35-f4d86907a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get MMCCs features\n",
    "mfcc_feature = [] \n",
    "for audiofile in audiofiles:\n",
    "    \n",
    "    signal, fs = librosa.load(audiopath + audiofile)\n",
    "    s_len = len(signal)\n",
    "\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
    "\n",
    "    feature = np.repeat(mfcc[:, :, np.newaxis], 10, axis=2)\n",
    "    feature = feature.T\n",
    "    mfcc_feature.append([feature, audiofile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e58ca1b-0a58-4f7a-bd93-c6a15a6c684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_feature_all = {}GoDiS\n",
    "mfcc_feature_all =  pd.DataFrame(mfcc_feature, columns = ['feature','filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471c841d-2988-436c-bb1a-2b773795c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[-797.5734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>11_face_B1_BA1_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[-782.4599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>11_face_B2_BA1_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[-425.25153, 121.934296, 12.968132, 34.21401...</td>\n",
       "      <td>11_face_B3_BA1_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[-461.3756, 93.42343, -12.372622, 31.628988,...</td>\n",
       "      <td>11_face_B4_BA1_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[-862.37775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>12_face_B1_BA1_trim_user.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  \\\n",
       "0  [[[-797.5734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
       "1  [[[-782.4599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
       "2  [[[-425.25153, 121.934296, 12.968132, 34.21401...   \n",
       "3  [[[-461.3756, 93.42343, -12.372622, 31.628988,...   \n",
       "4  [[[-862.37775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...   \n",
       "\n",
       "                       filename  \n",
       "0  11_face_B1_BA1_trim_user.wav  \n",
       "1  11_face_B2_BA1_trim_user.wav  \n",
       "2  11_face_B3_BA1_trim_user.wav  \n",
       "3  11_face_B4_BA1_trim_user.wav  \n",
       "4  12_face_B1_BA1_trim_user.wav  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_feature_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7851d2e4-e0b7-4efa-812b-1b4fd55ea87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature_all.to_csv(\"/home/nali/Develop/HRI/data/HRI_Study2_result_csv/participantSpeech/speechEmotionDetection/emotionspeech_features_part2_A1_B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205acf17-949e-4f13-aa38-b36d5f7ba684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "librosa.display.specshow(mfcc, sr=librosa_sr,x_axis='time')\n",
    "plt.colorbar(format='%+2.0f db')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "85209b6c-4895-4202-bec7-bc664848bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_loaded = np.load('/home/nali/Develop/HRI/SER MFCC Features/IEMOCAP.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2fba385-f53a-4f40-952e-6baae0fdf70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Test_Models/IEMOCAP_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58218375-1840-40c2-a33c-f4ba054fee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_CLASS_LABELS = (\"angry\",\"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\")#SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4a318900-1cc4-4e4a-838d-4e94f706bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IEMOCAP_CLASS_LABELS = (\"angry\", \"happy\", \"neutral\", \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21027511-7733-4afd-9993-40e12112e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAVDE_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")#rav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7a491e77-c605-4480-b52f-fe7528f321d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/nali/Develop/HRI/SER MFCC Features/IEMOCAP.npy\",allow_pickle=True).item()\n",
    "x_source_sample = data[\"x\"]\n",
    "y_source_sample = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2190db-c21a-4139-8cab-453743cdfd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_source = feature[newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eb05cc39-f9f2-4c15-aa21-997c8fe9a661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 107, 39)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80604056-3a7f-40a6-b819-5c25ee91bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee79d4-8b9d-47e8-b350-4433e0cae010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_new = np.repeat(mfcc[:, :, np.newaxis], 10, axis=2)\n",
    "# x_source = x_new.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33136ae8-46da-4b66-b637-964f9dbb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TIMNET_Model(input_shape=x_source.shape[1:], class_label=IEMOCAP_CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa609e-2b5b-4099-ba45-e87a55dd133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_feats, y_labels = model.test(x_source, y_source, path=path)# x_feats and y_labels are test datas for t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "78c18196-560e-417f-b96b-99ec2c2ccbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label =IEMOCAP_CLASS_LABELS\n",
    "num_classes = len(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c0a67144-06b8-41d3-b4a1-d8599d3c6ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 107, 39)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "66c6327a-b3ff-484b-add7-8457d2195063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source.shape[1:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c539eab-8f7d-49a1-8652-9fd1c83accf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_face_B1_BA2_trim_user.wav 0\n"
     ]
    }
   ],
   "source": [
    "for index, feature in mfcc_feature_all.iterrows():\n",
    "    print(feature['filename'], index)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "14d2a7dc-f100-4a77-8478-6dc89d496de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_source = np.zeros((10, 4), dtype=int)\n",
    "y_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835b2b0-b6be-43b4-b928-89dd201552b9",
   "metadata": {},
   "source": [
    "### Predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a49c52-9d69-4cb6-988b-54c71bc2eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True \n",
    "session = tf.compat.v1.Session(config=config)\n",
    "print(f\"###gpus:{gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec064bb-9515-4679-acf2-eca825064055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data_shape):\n",
    "    inputs=Input(shape = (data_shape[0],data_shape[1]))\n",
    "    multi_decision = TIMNET(nb_filters=39,\n",
    "                            kernel_size=2, \n",
    "                            nb_stacks=1,\n",
    "                            dilations=10, #if iemocap -> 10\n",
    "                            dropout_rate=0.1,\n",
    "                            activation = 'relu',\n",
    "                            return_sequences=True, \n",
    "                            name='TIMNET')(inputs)\n",
    "\n",
    "    decision = WeightLayer()(multi_decision)\n",
    "    predictions = Dense(num_classes, activation='softmax')(decision)\n",
    "    model = Model(inputs = inputs, outputs = predictions)\n",
    "\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                       optimizer =Adam(learning_rate=0.001, beta_1=0.93, beta_2=0.98, epsilon=1e-8),\n",
    "                       metrics = ['accuracy'])\n",
    "    # print(\"Temporal create succes!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaa61c-ee29-4f80-8c0b-18da551c69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, factor=0.1):\n",
    "    # smooth the labels\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    return labels\n",
    "\n",
    "class WeightLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1],1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)  \n",
    "        super(WeightLayer, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        tempx = tf.transpose(x,[0,2,1])\n",
    "        x = K.dot(tempx,self.kernel)\n",
    "        x = tf.squeeze(x,axis=-1)\n",
    "        return  x\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3fcc0-1f98-44a6-a572-d0ec9a23aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, feature in mfcc_feature_all.iterrows():\n",
    "    x_source = feature['feature']\n",
    "    i=0\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=16) #SAVEE random_seed 44 #imocap 16\n",
    "    avg_accuracy = 0\n",
    "    avg_loss = 0\n",
    "    x_feats = []\n",
    "    y_labels = []\n",
    "    matrix = []\n",
    "    x = x_source\n",
    "    y = y_source\n",
    "    y_pred_best_all = []\n",
    "    eva_matrix = []\n",
    "    y_best_index = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(x)):\n",
    "        print(\"hdf5 file index\",i)\n",
    "        model = create_model(x.shape[1:])\n",
    "        weight_path=path+'/'+str(10)+\"-fold_weights_best_\"+str(i+1)+\".hdf5\"\n",
    "        # model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = 64,epochs = 0,verbose=0)\n",
    "        model.fit(x_source_sample[train_index], y_source_sample[train_index],batch_size = 64,epochs = 0,verbose=0)\n",
    "        model.load_weights(weight_path)#+source_name+'_single_best.hdf5')\n",
    "        # best_eva_list = model.evaluate(x[test],  y[test])\n",
    "        #avg_loss += best_eva_list[0]\n",
    "        # avg_accuracy += best_eva_list[1]\n",
    "        # print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "        i+=1\n",
    "        y_pred_best = model.predict(x[test_index])\n",
    "        y_pred_best_all.append(y_pred_best)\n",
    "        y_best_index += np.argmax(y_pred_best)\n",
    "        # matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "        # em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=class_label,output_dict=True)\n",
    "        # eva_matrix.append(em)\n",
    "        # print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=class_label))\n",
    "        # caps_layer_model = Model(inputs=model.input,\n",
    "        # outputs=model.get_layer(index=-2).output)\n",
    "        # feature_source = caps_layer_model.predict(x[test])\n",
    "        # x_feats.append(feature_source)\n",
    "        # y_labels.append(y[test])\n",
    "    #print(\"Average ACC:\",avg_accuracy/10)\n",
    "    #acc = avg_accuracy/10\n",
    "    y_bast_index_best = y_best_index/10\n",
    "    results.append([y_bast_index_best, feature['filename']])\n",
    "    print(\"wavfile:\",index)\n",
    "print(\"done\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6646def2-cd58-4fca-a0a9-7b853f82857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10_face_B1_BA2_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>10_face_B2_BA2_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>10_face_B3_BA2_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10_face_B4_BA2_trim_user.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>14_face_B1_BA2_trim_user.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores                      filename\n",
       "0     0.1  10_face_B1_BA2_trim_user.wav\n",
       "1     0.4  10_face_B2_BA2_trim_user.wav\n",
       "2     0.3  10_face_B3_BA2_trim_user.wav\n",
       "3     0.0  10_face_B4_BA2_trim_user.wav\n",
       "4     0.6  14_face_B1_BA2_trim_user.wav"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_results =  pd.DataFrame(results, columns = ['scores','filename'])\n",
    "emotion_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2562c87b-e89a-499e-b249-3bf2d16c1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_results.to_csv(\"/home/nali/Develop/HRI/data/HRI_Study2_result_csv/participantSpeech/SpeechEmotionDetection/emotionspeech_results_part2_A2_B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece147dc-ebca-4edc-81c7-5f62b797da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42b5ac-cf7b-4ebb-8c02-ce9c1b9b78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred_best_all[9],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea6a8b-4c9a-4e45-b3c6-7f5316de07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred_best_all[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a237343-47f8-409e-a79d-e0f3e6ce20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(y_pred_best_all[0]) in (0, 1, 2, 3):\n",
    "                categoryscore = 0\n",
    "            elif np.argmax(scores) in (3, 6):\n",
    "                categoryscore = 1\n",
    "            else:\n",
    "                categoryscore = 2\n",
    "            df_emotion_frame.loc[index] = [fname, np.argmax(scores), categoryscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d6756-8d0a-4cd3-afca-bb1b893c7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred_best_all[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec333c46-228b-4027-ac9a-408d1b821ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred_best_all[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534ec27-926f-4fb1-8833-a40b0f0b5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TIMNET_Model(input_shape=x_source.shape[1:], class_label=IEMOCAP_CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf8c6f-c624-4c81-893e-ffcbc8e356f3",
   "metadata": {},
   "source": [
    "### Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbbece-c225-433a-bbb9-99e216463468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIMNET_Model(Common_Model):\n",
    "    def __init__(self, input_shape, class_label, **params):\n",
    "        super(TIMNET_Model,self).__init__(**params)\n",
    "        # self.args = args\n",
    "        self.data_shape = input_shape\n",
    "        self.num_classes = len(class_label)\n",
    "        self.class_label = class_label\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        print(\"TIMNET MODEL SHAPE:\",input_shape)\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.inputs=Input(shape = (self.data_shape[0],self.data_shape[1]))\n",
    "        self.multi_decision = TIMNET(nb_filters=39,\n",
    "                                kernel_size=2, \n",
    "                                nb_stacks=1,\n",
    "                                dilations=10, #if iemocap -> 10\n",
    "                                dropout_rate=0.1,\n",
    "                                activation = 'relu',\n",
    "                                return_sequences=True, \n",
    "                                name='TIMNET')(self.inputs)\n",
    "\n",
    "        self.decision = WeightLayer()(self.multi_decision)\n",
    "        self.predictions = Dense(self.num_classes, activation='softmax')(self.decision)\n",
    "        self.model = Model(inputs = self.inputs, outputs = self.predictions)\n",
    "        \n",
    "        self.model.compile(loss = \"categorical_crossentropy\",\n",
    "                           optimizer =Adam(learning_rate=0.001, beta_1=0.93, beta_2=0.98, epsilon=1e-8),\n",
    "                           metrics = ['accuracy'])\n",
    "        print(\"Temporal create succes!\")\n",
    "        return self.model\n",
    "    \n",
    "    def test(self, x, y, path):\n",
    "        i=1\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=44) #SAVEE random_seed 44\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        x_feats = []\n",
    "        y_labels = []\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            weight_path=path+'/'+str(10)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            self.model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = 64,epochs = 0,verbose=0)\n",
    "            self.model.load_weights(weight_path)#+source_name+'_single_best.hdf5')\n",
    "            best_eva_list = self.model.evaluate(x[test],  y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "            em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label,output_dict=True)\n",
    "            self.eva_matrix.append(em)\n",
    "            print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label))\n",
    "            caps_layer_model = Model(inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(index=-2).output)\n",
    "            feature_source = caps_layer_model.predict(x[test])\n",
    "            x_feats.append(feature_source)\n",
    "            y_labels.append(y[test])\n",
    "        print(\"Average ACC:\",avg_accuracy/10)\n",
    "        self.acc = avg_accuracy/10\n",
    "        return x_feats, y_labels "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
